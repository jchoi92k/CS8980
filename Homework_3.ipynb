{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO6oLFOheJb59tIrqCtvDeY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchoi92k/CS8980/blob/main/Homework_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qnK3pg5X-tV"
      },
      "source": [
        "Joon Suh Choi 002-51-1521"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRgaljbzYAGR"
      },
      "source": [
        "1. Using NLTK tokenize all documents, separated by polarity, remove stop words, and list\r\n",
        "the top 20 most frequent tokens (and their counts) for the positive reviews, and the top\r\n",
        "20 most frequent tokens (and their counts). What kind of things do you notice are\r\n",
        "different between the two sets? (30 points)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRZojkk7oHqg"
      },
      "source": [
        "!tar -xvf /content/review_polarity.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1QeavOmYCPc"
      },
      "source": [
        "from sklearn.datasets import load_files\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Load files and labels\r\n",
        "data = load_files('txt_sentoken', encoding='utf-8')\r\n",
        "data_neg = load_files('txt_sentoken', categories = 'neg', encoding='utf-8')\r\n",
        "data_pos = load_files('txt_sentoken', categories= 'pos', encoding='utf-8')\r\n",
        "data_pos.target = np.asarray([1 for w in range(len(data_pos.target))])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVTI6DDWfdiY",
        "outputId": "a35cf023-c02a-4282-d8a2-0ab7167460b6"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.corpus import stopwords  \r\n",
        "from nltk.tokenize import word_tokenize  \r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "stop_words = set(stopwords.words('english'))  \r\n",
        "\r\n",
        "tokens_neg = []\r\n",
        "tokens_pos = []\r\n",
        "\r\n",
        "for text in data_neg.data:\r\n",
        "  word_tokens = word_tokenize(text)  \r\n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words]\r\n",
        "  tokens_neg += filtered_sentence\r\n",
        "\r\n",
        "for text in data_pos.data:\r\n",
        "  word_tokens = word_tokenize(text)  \r\n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words]\r\n",
        "  tokens_pos += filtered_sentence"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV2bAX1co19p",
        "outputId": "afba500a-f265-4950-e3f9-15bc62308d8b"
      },
      "source": [
        "from collections import Counter\r\n",
        "\r\n",
        "# For positive tokens\r\n",
        "Counter(tokens_pos).most_common(20)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 42448),\n",
              " ('.', 33714),\n",
              " (\"'s\", 9473),\n",
              " ('``', 8494),\n",
              " (')', 6039),\n",
              " ('(', 6014),\n",
              " ('film', 5186),\n",
              " ('one', 2943),\n",
              " (\"n't\", 2775),\n",
              " ('movie', 2497),\n",
              " ('like', 1713),\n",
              " ('?', 1570),\n",
              " (':', 1502),\n",
              " ('story', 1231),\n",
              " ('also', 1200),\n",
              " ('good', 1190),\n",
              " ('even', 1175),\n",
              " ('time', 1171),\n",
              " ('would', 1079),\n",
              " ('character', 1067)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNONwLp8r_hG",
        "outputId": "a88f3eb4-2f0c-4b2f-e1e7-d67289457aa9"
      },
      "source": [
        "# For negative tokens\r\n",
        "Counter(tokens_neg).most_common(20)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 35269),\n",
              " ('.', 32162),\n",
              " ('``', 9123),\n",
              " (\"'s\", 8655),\n",
              " (')', 5742),\n",
              " ('(', 5650),\n",
              " ('film', 4257),\n",
              " (\"n't\", 3442),\n",
              " ('movie', 3174),\n",
              " ('one', 2637),\n",
              " ('?', 2201),\n",
              " ('like', 1832),\n",
              " (':', 1540),\n",
              " ('even', 1381),\n",
              " ('would', 1185),\n",
              " ('good', 1126),\n",
              " ('time', 1111),\n",
              " ('!', 1056),\n",
              " ('get', 1039),\n",
              " ('bad', 1019)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK5D-nxpsNJE"
      },
      "source": [
        "To more clearly see the differences between the two lists I used normalized (per 1000 words) lists which are shown below. The token [good] appeared in both lists: the raw frequency was slightly higher in pos which seemed to be as expected, but the normalized frequency revealed that [good] was actually less frequent in positive reviews (although the difference seems negligible). Exclamation point only appeared frequently in negative reviews, which probably indicates that stronger opinions were expressed in them compared to positive reviews. The word [bad] also only appeared frequently in negative reviews, which is as expected. The word [character] appeared in the positive list only, suggesting that if a review discusses characters, it is likely to be on a positive note. The token [n't] appeared more frequently in negative texts, which is somewhat expected considering that it is a contraction of the negation word [not]. The word [also] appeared in the positive list only, which suggests that either 1) something other than also was used frequently in negative reviews as transition words, or 2) maybe there was less trasition in negative reviews and criticism focused more on a single element while positive comments were spread across different elements. A more detailed analysis would prove or disprove this hypothesis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXwaOb7EaCZ5",
        "outputId": "472b5112-c51e-427a-975f-5eab0c265255"
      },
      "source": [
        "# Normalized per 1000 tokens: positive reviews\r\n",
        "for w in Counter(tokens_pos).most_common(20):\r\n",
        "  print(f'(\"{w[0]}\", {w[1]/len(tokens_pos) * 1000})')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(\",\", 86.79365650584377)\n",
            "(\".\", 68.93519919520394)\n",
            "(\"'s\", 19.369494630603516)\n",
            "(\"``\", 17.367728005103583)\n",
            "(\")\", 12.34797615055575)\n",
            "(\"(\", 12.296858514562391)\n",
            "(\"film\", 10.603842410462349)\n",
            "(\"one\", 6.017568109138198)\n",
            "(\"n't\", 5.674057595262827)\n",
            "(\"movie\", 5.105629483016677)\n",
            "(\"like\", 3.502580418264945)\n",
            "(\"?\", 3.2101875403829325)\n",
            "(\":\", 3.0711475704809965)\n",
            "(\"story\", 2.517032396312987)\n",
            "(\"also\", 2.4536465276812223)\n",
            "(\"good\", 2.433199473283879)\n",
            "(\"even\", 2.4025288916878633)\n",
            "(\"time\", 2.394350069928926)\n",
            "(\"would\", 2.206237169473366)\n",
            "(\"character\", 2.1817007041965533)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4XDFdIPaDZr",
        "outputId": "224ca0ea-2b99-4a75-93fb-10865c1df8a3"
      },
      "source": [
        "# Normalized per 1000 tokens: negative reviews\r\n",
        "for w in Counter(tokens_neg).most_common(20):\r\n",
        "  print(f'(\"{w[0]}\", {w[1]/len(tokens_neg) * 1000})')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(\",\", 80.13132185213796)\n",
            "(\".\", 73.07220429863226)\n",
            "(\"``\", 20.72749579679193)\n",
            "(\"'s\", 19.66419775525969)\n",
            "(\")\", 13.045849048030172)\n",
            "(\"(\", 12.836824646703322)\n",
            "(\"film\", 9.671922570091334)\n",
            "(\"n't\", 7.820239014858909)\n",
            "(\"movie\", 7.211341845776343)\n",
            "(\"one\", 5.991275503248966)\n",
            "(\"?\", 5.000681601308675)\n",
            "(\"like\", 4.162311991639023)\n",
            "(\":\", 3.498886717862498)\n",
            "(\"even\", 3.1376380242650064)\n",
            "(\"would\", 2.692325169264325)\n",
            "(\"good\", 2.558276911891671)\n",
            "(\"time\", 2.5241968464579454)\n",
            "(\"!\", 2.3992366065342847)\n",
            "(\"get\", 2.3606125323760625)\n",
            "(\"bad\", 2.3151724451310947)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD41wBrWsxRp"
      },
      "source": [
        "2. Using the code from previous lectures, build 3 polarity classifiers using the following\r\n",
        "parameters (20 points). Note: just train the models.\r\n",
        "a) For training: use 50% of the positive dataset and 70% of the negative dataset. For\r\n",
        "your model use: NaiveBayes with the TF-IDF vectorizer.\r\n",
        "b) For training: use 50% of the negative dataset and 70% of the positive dataset. For\r\n",
        "your model use: NaiveBayes with the TF-IDF vectorizer.\r\n",
        "c) For training: use 25% of the negative dataset and 25% of the positive dataset. For\r\n",
        "your model use: SVM with the TF-IDF vectorizer.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzXyauH1s1V7",
        "outputId": "6a0930cb-744c-42f9-90ba-ad528243dbbb"
      },
      "source": [
        "%matplotlib inline\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random\r\n",
        "import sklearn.metrics\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.svm import SVC\r\n",
        "\r\n",
        "random.seed(12345)\r\n",
        "\r\n",
        "### Train Model A\r\n",
        "\r\n",
        "# Create train and test set using train_test_split and concatenate\r\n",
        "apx_train, apx_test, apy_train, apy_test = train_test_split(data_pos.data, data_pos.target, train_size=0.5)\r\n",
        "anx_train, anx_test, any_train, any_test = train_test_split(data_neg.data, data_neg.target, train_size=0.7)\r\n",
        "\r\n",
        "ax_train = apx_train + anx_train\r\n",
        "ax_test = apx_test + anx_test\r\n",
        "ay_train = np.concatenate((apy_train, any_train))\r\n",
        "ay_test = np.concatenate((apy_test, any_test))\r\n",
        "\r\n",
        "# Actual training\r\n",
        "model_a = make_pipeline(TfidfVectorizer(), MultinomialNB())\r\n",
        "model_a.fit(ax_train, ay_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('multinomialnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyOwzQl2khRh",
        "outputId": "cd943720-ae85-4ac2-e036-523f4b3cdcc0"
      },
      "source": [
        "### Train Model B\r\n",
        "\r\n",
        "random.seed(12345)\r\n",
        "\r\n",
        "# Create train and test set\r\n",
        "bpx_train, bpx_test, bpy_train, bpy_test = train_test_split(data_pos.data, data_pos.target, train_size=0.7)\r\n",
        "bnx_train, bnx_test, bny_train, bny_test = train_test_split(data_neg.data, data_neg.target, train_size=0.5)\r\n",
        "\r\n",
        "bx_train = bpx_train + bnx_train\r\n",
        "bx_test = bpx_test + bnx_test\r\n",
        "by_train = np.concatenate((bpy_train, bny_train))\r\n",
        "by_test = np.concatenate((bpy_test, bny_test))\r\n",
        "\r\n",
        "# Actual training\r\n",
        "model_b = make_pipeline(TfidfVectorizer(), MultinomialNB())\r\n",
        "model_b.fit(bx_train, by_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('multinomialnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NFHWSuMlyMC",
        "outputId": "2d17b3cf-8bf3-4976-a35a-717e405262a0"
      },
      "source": [
        "### Train Model C\r\n",
        "\r\n",
        "random.seed(12345)\r\n",
        "\r\n",
        "# Create train and test set\r\n",
        "cpx_train, cpx_test, cpy_train, cpy_test = train_test_split(data_pos.data, data_pos.target, train_size=0.25)\r\n",
        "cnx_train, cnx_test, cny_train, cny_test = train_test_split(data_neg.data, data_neg.target, train_size=0.25)\r\n",
        "\r\n",
        "cx_train = cpx_train + cnx_train\r\n",
        "cx_test = cpx_test + cnx_test\r\n",
        "cy_train = np.concatenate((cpy_train, cny_train))\r\n",
        "cy_test = np.concatenate((cpy_test, cny_test))\r\n",
        "\r\n",
        "# Actual training\r\n",
        "model_c = make_pipeline(TfidfVectorizer(), SVC())\r\n",
        "model_c.fit(cx_train, cy_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('svc',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIOwg2-us1kE"
      },
      "source": [
        "3. Using the models from question 2, evaluate them on their individual rest of the dataset.\r\n",
        "This is, for a) 50% positive and 30% negative, for b) 50% negative and 30% positive, and\r\n",
        "for c) 75% negative and 75% positive. Calculate and show ONLY the following metrics for\r\n",
        "each model: Accuracy, Precision, Recall, Macro F1-score. (15 points)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8Ztf8Fps44V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da77dc70-79a3-4ac1-8fb6-933d77340bc9"
      },
      "source": [
        "# Evaluate Model A\r\n",
        "labels_a = model_a.predict(ax_test)\r\n",
        "\r\n",
        "print('Accuracy:', sklearn.metrics.accuracy_score(ay_test, labels_a))\r\n",
        "print('Precision:', sklearn.metrics.precision_score(ay_test, labels_a))\r\n",
        "print('Recall:', sklearn.metrics.recall_score(ay_test, labels_a))\r\n",
        "print('F1 Score:', sklearn.metrics.f1_score(labels_a, ay_test, average='macro'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.3775\n",
            "Precision: 1.0\n",
            "Recall: 0.004\n",
            "F1 Score: 0.2772081074608669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qYc6RXqti53",
        "outputId": "16c567b2-9912-4b53-992a-e1a6b58263d5"
      },
      "source": [
        "# Evaluate Model B\r\n",
        "labels_b = model_b.predict(bx_test)\r\n",
        "\r\n",
        "print('Accuracy:', sklearn.metrics.accuracy_score(by_test, labels_b))\r\n",
        "print('Precision:', sklearn.metrics.precision_score(by_test, labels_b))\r\n",
        "print('Recall:', sklearn.metrics.recall_score(by_test, labels_b))\r\n",
        "print('F1 Score:', sklearn.metrics.f1_score(labels_b, by_test, average='macro'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.375\n",
            "Precision: 0.375\n",
            "Recall: 1.0\n",
            "F1 Score: 0.2727272727272727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOOjWK9ptoHv",
        "outputId": "57e2fb77-b5f2-4cf0-f761-c91a08f58f35"
      },
      "source": [
        "# Evaluate Model C\r\n",
        "labels_c = model_c.predict(cx_test)\r\n",
        "\r\n",
        "print('Accuracy:', sklearn.metrics.accuracy_score(cy_test, labels_c))\r\n",
        "print('Precision:', sklearn.metrics.precision_score(cy_test, labels_c))\r\n",
        "print('Recall:', sklearn.metrics.recall_score(cy_test, labels_c))\r\n",
        "print('F1 Score:', sklearn.metrics.f1_score(labels_c, cy_test, average='macro'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7833333333333333\n",
            "Precision: 0.7906976744186046\n",
            "Recall: 0.7706666666666667\n",
            "F1 Score: 0.783298564791951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O1tGIGTt7G0"
      },
      "source": [
        "4) Using the model performance metrics from question 3, answer the following\r\n",
        "questions. Please provide logical and intuitive rationale for your answers, simple\r\n",
        "answers like: because it has the best score, will not be sufficient. (40 points):\r\n",
        "a) What is the best performing model?\r\n",
        "b) Why do you think this is the best performing model?\r\n",
        "c) How does class imbalance play in determining polarity?\r\n",
        "d) Do you think either more data or a better model is a better approach for this\r\n",
        "kind of task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_pytvl6s5KO"
      },
      "source": [
        "a) The best performing model is Model C because it performs reasonably well in all metrics (i.e., accuracy, precision, recall, and F1 score) while the other two models excel in only one metric (i.e., precision for Model A and recall for model B).\r\n",
        "\r\n",
        "b) Model C is the best performing model because it has a better data balance than the other models (i.e., same ratio between positive and negative data). Also, SVM may have outperformed Naive Bayes (which assumes independence between data), because the reviews are not entirely independent from one another (because a reviewer may have read another review - not necessarily for the same movie - and that may have affected the newly created review).\r\n",
        "\r\n",
        "c) More negative data translates to higher precision and lower recall, and more positive data translates to higher recall and lower precision. The way the model is set up currently, higher precision means that more texts were labeled as negative than the true labels, and higher recall means that more texts were labeled as positive compared to the the true labels. This is expected because the model will train itself to label texts more frequently into the bigger portion of the train data set.\r\n",
        "\r\n",
        "d) I believe building a better model could be more beneficial over collecting more data because a 1 million token corpus (for both positive and negative texts in the txt_sentoken folder) seems reasonably big enough for the task of binary classification. It could also be beneficial to look at the thresholds as well as some ambiguous data being fed to the model. Looking at the Readme file for this dataset, the labels were created using a threshold of 4/10 for negative reviews and 7/10 for positive reviews. Tinkering with these thresholds or working with the unlabeled texts to create an improved set of labels could be a better objective than procuring more data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVPN8Ai7uHjG"
      },
      "source": [
        "5) Using NLTK and VADER, calculate the sentiment score for all documents in the\r\n",
        "positive polarity. Calculate the polarity threshold needed (and reasonable) to have the\r\n",
        "majority of the document labels match. Do the same for the negative class. Provide the\r\n",
        "threshold needed, the reason why you think this threshold is reasonable, and the\r\n",
        "accuracy percentage (how many documents are correctly labeled using this threshold).\r\n",
        "(45 points):\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsRwxZTguIKi",
        "outputId": "bc3596af-b6d5-4db0-d7b3-6674f4a24285"
      },
      "source": [
        "import nltk\r\n",
        "\r\n",
        "# Download Vader\r\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO4wef0Hv_Xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a84e0f-60d0-4f1d-ee1d-284455120615"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\r\n",
        "\r\n",
        "sia = SIA()\r\n",
        "\r\n",
        "# Get polarity scores for both positive and negative items\r\n",
        "positems = [sia.polarity_scores(w)['compound'] for w in data_pos.data]\r\n",
        "negitems = [sia.polarity_scores(w)['compound'] for w in data_neg.data]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cn30tcp6T6r",
        "outputId": "3a9194b5-6fdd-452f-e763-c98f3b5921a0"
      },
      "source": [
        "from sklearn.model_selection import KFold\r\n",
        "\r\n",
        "random.seed(12345)\r\n",
        "\r\n",
        "# If we are considering accuracy only (i.e., not precision, recall, or F1)\r\n",
        "\r\n",
        "# For Kfold cross validation\r\n",
        "kf = KFold(n_splits = 10)\r\n",
        "\r\n",
        "positems=np.array(positems)\r\n",
        "negitems=np.array(negitems)\r\n",
        "\r\n",
        "batchn = 0\r\n",
        "# To average the optimal thresholds found in each fold\r\n",
        "allthresh = []\r\n",
        "\r\n",
        "# Split data using kf.split\r\n",
        "for (posfolds_train, posfolds_test), (negfolds_train, negfolds_test) in zip(kf.split(positems), kf.split(negitems)):\r\n",
        "  batchn += 1\r\n",
        "  X_train = np.concatenate((positems[posfolds_train], negitems[negfolds_train]))\r\n",
        "  y_train =  [1 for w in range(len(posfolds_train))] + [0 for w in range(len(negfolds_train))]\r\n",
        "  X_test = np.concatenate((positems[posfolds_test], negitems[negfolds_test]))\r\n",
        "  y_test =  [1 for w in range(len(posfolds_test))] + [0 for w in range(len(negfolds_test))]\r\n",
        "\r\n",
        "  # For finding the optimal threshold\r\n",
        "  opt = 0\r\n",
        "  thrsh = 0\r\n",
        "\r\n",
        "  for p in np.arange(-1, 1, 0.001):\r\n",
        "    crr = 0\r\n",
        "    fll = len(X_train)\r\n",
        "    for a, b in zip(X_train, y_train):\r\n",
        "      if (a > p and b == 1) or (a < p and b == 0):\r\n",
        "        crr += 1\r\n",
        "      else:\r\n",
        "        pass\r\n",
        "    if crr/fll > opt:\r\n",
        "      opt = crr/fll\r\n",
        "      thrsh = p\r\n",
        "\r\n",
        "  # Evaluating the result on the test set of each fold\r\n",
        "  crr = 0\r\n",
        "  fll = len(X_test)\r\n",
        "  for a, b in zip(X_test, y_test):\r\n",
        "    if (a > p and b == 1) or (a < p and b == 0):\r\n",
        "      crr += 1\r\n",
        "    else:\r\n",
        "      pass\r\n",
        "  opt = crr/fll\r\n",
        "  \r\n",
        "  allthresh.append(thrsh)\r\n",
        "  print(f'For batch number {batchn}, threshold is: {thrsh}, and accuracy of test set lables is: {opt}')\r\n",
        "\r\n",
        "# For final evaluation on complete set\r\n",
        "completeset = np.concatenate((positems[posfolds_train], negitems[negfolds_train]))\r\n",
        "completelabels = [1 for w in range(len(positems))] + [0 for w in range(len(negitems))]\r\n",
        "finthresh = sum(allthresh)/len(allthresh)\r\n",
        "\r\n",
        "crr = 0\r\n",
        "fll = len(completeset)\r\n",
        "for a, b in zip(completeset, completelabels):\r\n",
        "  if (a > finthresh and b == 1) or (a < finthresh and b == 0):\r\n",
        "    crr += 1\r\n",
        "  else:\r\n",
        "    pass\r\n",
        "if crr/fll > opt:\r\n",
        "  opt = crr/fll\r\n",
        "\r\n",
        "print(f'Final threshold is {finthresh} and accuarcy of the entire dataset is {opt}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For batch number 1, threshold is: 0.9740000000000018, and accuracy of test set lables is: 0.545\n",
            "For batch number 2, threshold is: 0.9740000000000018, and accuracy of test set lables is: 0.52\n",
            "For batch number 3, threshold is: 0.9740000000000018, and accuracy of test set lables is: 0.52\n",
            "For batch number 4, threshold is: 0.9710000000000019, and accuracy of test set lables is: 0.555\n",
            "For batch number 5, threshold is: 0.9740000000000018, and accuracy of test set lables is: 0.54\n",
            "For batch number 6, threshold is: 0.9740000000000018, and accuracy of test set lables is: 0.545\n",
            "For batch number 7, threshold is: 0.9710000000000019, and accuracy of test set lables is: 0.56\n",
            "For batch number 8, threshold is: 0.9740000000000018, and accuracy of test set lables is: 0.525\n",
            "For batch number 9, threshold is: 0.9740000000000018, and accuracy of test set lables is: 0.54\n",
            "For batch number 10, threshold is: 0.9740000000000018, and accuracy of test set lables is: 0.53\n",
            "Final threshold is 0.9734000000000018 and accuarcy of the entire dataset is 0.6683333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIRGteNdBeT2"
      },
      "source": [
        "The threshold I found is 0.9734. I think this is reasonable if we are only assessing accuracy because it is a threshold that has been derived using 10-fold cross validation. The accuracy percentage is 66.833. Using other measures such as g-mean or F1 score as the yardstick will result in different thresholds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi22ZO1cuIeE"
      },
      "source": [
        "Bonus (40 points): Repeat questions 2,3 and 4 removing all stopwords. Answer the\r\n",
        "following questions: Did this change the results in any way? Why do you think so?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HchWdg5_xk6w",
        "outputId": "c1d430d6-8e79-4d96-d976-e758b25b20fa"
      },
      "source": [
        "random.seed(12345)\r\n",
        "\r\n",
        "tokens_neg = []\r\n",
        "tokens_pos = []\r\n",
        "\r\n",
        "for text in data_neg.data:\r\n",
        "  word_tokens = word_tokenize(text)  \r\n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words]\r\n",
        "  tokens_neg.append(filtered_sentence)\r\n",
        "\r\n",
        "for text in data_pos.data:\r\n",
        "  word_tokens = word_tokenize(text)  \r\n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words]\r\n",
        "  tokens_pos.append(filtered_sentence)\r\n",
        "\r\n",
        "# post-stopword-removal texts\r\n",
        "postext_ns = [' '.join(w) for w in tokens_pos]\r\n",
        "negtext_ns = [' '.join(w) for w in tokens_neg]\r\n",
        "\r\n",
        "random.seed(12345)\r\n",
        "\r\n",
        "### Train Model A\r\n",
        "\r\n",
        "# Create train and test set using train_test_split and concatenate\r\n",
        "apx_train, apx_test, apy_train, apy_test = train_test_split(postext_ns, data_pos.target, train_size=0.5)\r\n",
        "anx_train, anx_test, any_train, any_test = train_test_split(negtext_ns, data_neg.target, train_size=0.7)\r\n",
        "\r\n",
        "ax_train = apx_train + anx_train\r\n",
        "ax_test = apx_test + anx_test\r\n",
        "ay_train = np.concatenate((apy_train, any_train))\r\n",
        "ay_test = np.concatenate((apy_test, any_test))\r\n",
        "\r\n",
        "# Actual training\r\n",
        "model_a = make_pipeline(TfidfVectorizer(), MultinomialNB())\r\n",
        "model_a.fit(ax_train, ay_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('multinomialnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0h2Lk1ly_9_",
        "outputId": "bbbc9af7-86fc-45cc-ddf3-a8cfc7dc68c5"
      },
      "source": [
        "### Train Model B\r\n",
        "\r\n",
        "random.seed(12345)\r\n",
        "\r\n",
        "# Create train and test set\r\n",
        "bpx_train, bpx_test, bpy_train, bpy_test = train_test_split(postext_ns, data_pos.target, train_size=0.7)\r\n",
        "bnx_train, bnx_test, bny_train, bny_test = train_test_split(negtext_ns, data_neg.target, train_size=0.5)\r\n",
        "\r\n",
        "bx_train = bpx_train + bnx_train\r\n",
        "bx_test = bpx_test + bnx_test\r\n",
        "by_train = np.concatenate((bpy_train, bny_train))\r\n",
        "by_test = np.concatenate((bpy_test, bny_test))\r\n",
        "\r\n",
        "# Actual training\r\n",
        "model_b = make_pipeline(TfidfVectorizer(), MultinomialNB())\r\n",
        "model_b.fit(bx_train, by_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('multinomialnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZt8II4aylBx",
        "outputId": "e4bafe5d-f702-4230-d91d-2ba688197c73"
      },
      "source": [
        "### Train Model C\r\n",
        "\r\n",
        "random.seed(12345)\r\n",
        "\r\n",
        "# Create train and test set\r\n",
        "cpx_train, cpx_test, cpy_train, cpy_test = train_test_split(postext_ns, data_pos.target, train_size=0.25)\r\n",
        "cnx_train, cnx_test, cny_train, cny_test = train_test_split(negtext_ns, data_neg.target, train_size=0.25)\r\n",
        "\r\n",
        "cx_train = cpx_train + cnx_train\r\n",
        "cx_test = cpx_test + cnx_test\r\n",
        "cy_train = np.concatenate((cpy_train, cny_train))\r\n",
        "cy_test = np.concatenate((cpy_test, cny_test))\r\n",
        "\r\n",
        "# Actual training\r\n",
        "model_c = make_pipeline(TfidfVectorizer(), SVC())\r\n",
        "model_c.fit(cx_train, cy_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('svc',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO_wHOKFzUjb",
        "outputId": "7705aa14-7d5a-40b3-ad98-ef81b5ebdad1"
      },
      "source": [
        "# Evaluate Model A\r\n",
        "labels_a = model_a.predict(ax_test)\r\n",
        "\r\n",
        "print('Accuracy:', sklearn.metrics.accuracy_score(ay_test, labels_a))\r\n",
        "print('Precision:', sklearn.metrics.precision_score(ay_test, labels_a))\r\n",
        "print('Recall:', sklearn.metrics.recall_score(ay_test, labels_a))\r\n",
        "print('F1 Score:', sklearn.metrics.f1_score(labels_a, ay_test, average='macro'))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.3925\n",
            "Precision: 1.0\n",
            "Recall: 0.028\n",
            "F1 Score: 0.3034804480082551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcjAM93EzanB",
        "outputId": "ad7d0828-acb0-4ceb-e3c8-05c5d4471bd2"
      },
      "source": [
        "# Evaluate Model B\r\n",
        "labels_b = model_b.predict(bx_test)\r\n",
        "\r\n",
        "print('Accuracy:', sklearn.metrics.accuracy_score(by_test, labels_b))\r\n",
        "print('Precision:', sklearn.metrics.precision_score(by_test, labels_b))\r\n",
        "print('Recall:', sklearn.metrics.recall_score(by_test, labels_b))\r\n",
        "print('F1 Score:', sklearn.metrics.f1_score(labels_b, by_test, average='macro'))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.3775\n",
            "Precision: 0.37593984962406013\n",
            "Recall: 1.0\n",
            "F1 Score: 0.2772081074608669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCLXfCXFzcEF",
        "outputId": "9b732453-719d-4e61-92c3-a7696f128ec0"
      },
      "source": [
        "# Evaluate Model C\r\n",
        "labels_c = model_c.predict(cx_test)\r\n",
        "\r\n",
        "print('Accuracy:', sklearn.metrics.accuracy_score(cy_test, labels_c))\r\n",
        "print('Precision:', sklearn.metrics.precision_score(cy_test, labels_c))\r\n",
        "print('Recall:', sklearn.metrics.recall_score(cy_test, labels_c))\r\n",
        "print('F1 Score:', sklearn.metrics.f1_score(labels_c, cy_test, average='macro'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7773333333333333\n",
            "Precision: 0.7549019607843137\n",
            "Recall: 0.8213333333333334\n",
            "F1 Score: 0.7769014144717507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p34lPsLmzg-q"
      },
      "source": [
        "a) The best performing model is still Model C for the same reasons as stated above (i.e., better overall scores including F1 Score).\r\n",
        "\r\n",
        "b) I think Model C remains the best performing model because the two relevant variables (i.e., ratio between pos/neg data being fed to the model, and SVM over Naive Bayes) did not change.\r\n",
        "\r\n",
        "c) Same as before: more negative data translates to higher precision and more positive data translates to higher recall.\r\n",
        "\r\n",
        "d) I believe building a better model could be more beneficial over collecting more data because a 1 million token corpus (for both positive and negative texts in the txt_sentoken folder) for the same reasons as stated above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3IBqAysGqTq"
      },
      "source": [
        "While Model C is still the best-performing model out of the three, its performance metrics (with the exception of recall scores) drops slightly compared to the older Model C which was trained on a corpus pre-stopword-removal. This could either be 1) due to chance considering the size of the differences, and a different sampling with a different seed will produce different results, or 2) an indication that stop words, while considered neutral in textblob and SIA as an individual token, perhaps contains some information about the polarity of the text they appear in. For instance, it was shown from the normalized word frequency lists in Q1 that the token [would] appeared more frequently in negative reviews than in positive reviews in this particular corpus. Perhaps this has to do with writing style, or whether some tokens appear more frequently as a component of ngrams, p-frames or other phraseological units."
      ]
    }
  ]
}